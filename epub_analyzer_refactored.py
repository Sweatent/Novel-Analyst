import ebooklib
from ebooklib import epub
from bs4 import BeautifulSoup
import requests
import json
import logging
import time
import re
from typing import List, Dict, Optional, Any
import os
from datetime import datetime
import signal
import sys
import pickle
from pathlib import Path
import traceback
from functools import wraps
import argparse
import subprocess
import platform

# ==============================================================================
# 1. é…ç½®ä¸æ—¥å¿—
# ==============================================================================

logging.basicConfig(
    level=logging.INFO,
    format='%(asctime)s - %(levelname)s - %(message)s',
    handlers=[
        logging.FileHandler('novel_analysis_refactored.log', encoding='utf-8'),
        logging.StreamHandler()
    ]
)
logger = logging.getLogger(__name__)

CONFIG_FILE = "config.json"

def load_config_from_file(file_path: str) -> Dict[str, Any]:
    """ä»JSONæ–‡ä»¶åŠ è½½é…ç½®"""
    if not Path(file_path).exists():
        return {}
    try:
        with open(file_path, 'r', encoding='utf-8') as f:
            return json.load(f)
    except (json.JSONDecodeError, IOError) as e:
        logger.error(f"åŠ è½½é…ç½®æ–‡ä»¶ {file_path} å¤±è´¥: {e}")
        return {}

def save_config_to_file(file_path: str, config_data: Dict[str, Any]):
    """å°†é…ç½®ä¿å­˜åˆ°JSONæ–‡ä»¶"""
    try:
        with open(file_path, 'w', encoding='utf-8') as f:
            json.dump(config_data, f, ensure_ascii=False, indent=4)
        logger.info(f"é…ç½®å·²ä¿å­˜åˆ° {file_path}")
    except IOError as e:
        logger.error(f"ä¿å­˜é…ç½®æ–‡ä»¶ {file_path} å¤±è´¥: {e}")

class Config:
    """ç»Ÿä¸€ç®¡ç†æ‰€æœ‰é…ç½®"""
    def __init__(self, settings: Dict[str, Any]):
        # ä»å­—å…¸åŠ è½½é…ç½®ï¼Œæä¾›é»˜è®¤å€¼
        self.api_key: str = settings.get("api_key")
        self.base_url: str = settings.get("base_url", "https://api.openai.com/v1")
        self.model: str = settings.get("model", "gpt-3.5-turbo")
        self.max_tokens: int = settings.get("max_tokens", 3000)
        self.api_timeout: float = settings.get("timeout", 180.0)
        self.rate_limit_delay: float = settings.get("delay", 2.0)
        self.max_retries: int = 5
        self.retry_delay: float = 3.0
        self.backoff: float = 1.5
        
        self.epub_path: str = settings.get("epub_path")
        self.output_path: str = settings.get("output", "analysis_results_refactored.json")
        self.report_path: str = settings.get("report", "analysis_report_refactored.md")
        self.progress_file: str = "analysis_progress_refactored.pkl"
        
        self.chunk_size: int = 150 # æ¯150ç« åˆ‡å‰²ä¸€æ¬¡
        self.long_chapter_threshold: int = 6000 # é•¿ç« èŠ‚åˆ†æ®µé˜ˆå€¼
        
        self.prompt: str = settings.get("prompt", self._get_default_prompt())
        self.use_json_schema: bool = settings.get("use_json_schema", False)

    @staticmethod
    def from_args(args: argparse.Namespace) -> 'Config':
        """ä»argparse.Namespaceå¯¹è±¡åˆ›å»ºConfigå®ä¾‹"""
        return Config(vars(args))

    def to_dict(self) -> Dict[str, Any]:
        """å°†é…ç½®è½¬æ¢ä¸ºå¯åºåˆ—åŒ–çš„å­—å…¸"""
        return {
            "api_key": self.api_key,
            "base_url": self.base_url,
            "model": self.model,
            "max_tokens": self.max_tokens,
            "timeout": self.api_timeout,
            "delay": self.rate_limit_delay,
            "epub_path": self.epub_path,
            "output": self.output_path,
            "report": self.report_path,
            "prompt": self.prompt,
            "use_json_schema": self.use_json_schema,
        }

    def _get_default_prompt(self) -> str:
        return """
ä½ ç°åœ¨éœ€è¦åˆ†æè¿™ç¯‡å†…å®¹å¹¶å®Œæˆä¸‹é¢çš„ä»»åŠ¡ï¼Œä½ éœ€è¦é€‰å–äººç”Ÿæ„Ÿæ‚Ÿæˆ–è€…æ˜¯æ‹çˆ±æ–¹æ³•ç»éªŒç­‰ï¼Œä¿ç•™ä¸Šä¸‹æ–‡ã€‚å¦‚æœæ²¡æœ‰å¯ä»¥å†é‡‘å¥é‚£é‡Œè®¾ç½®ä¸€ä¸ªç©ºåˆ—è¡¨
è¯·æ³¨æ„ï¼š
1.è¯·å‹¿åŒ…å«æ•…äº‹æƒ…èŠ‚ï¼šæ ·ä¾‹ï¼š
1>è™½ç„¶ä»–ä¸çŸ¥é“åˆ«çš„å°è¯´ä½œè€…æ˜¯æ€ä¹ˆæ ·çš„ï¼Œä½†ä»–å¹³æ—¶å°±å–œæ¬¢å»æ›´å¤šçš„åœ°æ–¹ï¼Œè§æ›´å¤šçš„é£æ™¯ï¼Œè¿™æ ·æ‰èƒ½å†™å‡ºæ¥æ›´åŠ ç²¾å½©çš„æ•…äº‹ã€‚
2>ä¸çŸ¥ä¸è§‰ä¸­ï¼Œè‹ç™½ç²¥çš„å¿ƒæƒ…å˜å¾—ååˆ†å®‰é€¸ï¼Œä¹‹å‰çš„ä¸æ„‰å¿«å·²ç»çƒŸæ¶ˆäº‘æ•£ã€‚
2.ä½ çš„ç›®çš„æ˜¯ä¿å­˜é‡‘å¥è®©äººæ„Ÿè§‰æœ‰ä»·å€¼çš„ï¼Œä¸æ˜¯ä¿ç•™äººç‰©çš„æ— ç”¨å¯¹è¯ç­‰
è¯·ä¸¥æ ¼éµå®ˆä»¥ä¸‹JSONæ ¼å¼è¦æ±‚ï¼ˆè¯·å‹¿åœ¨JSONå†…éƒ¨æ·»åŠ ä»»ä½•æ³¨é‡Šï¼‰ï¼š

<JSON>
{
    "chapter_title": "åœ¨æ­¤å¤„å¡«å†™ç« èŠ‚æ ‡é¢˜",
    "golden_sentences": [
        {
            "sentence": "åœ¨æ­¤å¤„å¡«å†™é»„é‡‘å¥å­çš„çº¯æ–‡æœ¬å†…å®¹",
            "speaker": "åœ¨æ­¤å¤„å¡«å†™å¥å­æ¥æºï¼ˆè§’è‰²å/æ—ç™½ç­‰ï¼Œä½ ä¹Ÿå¯ä»¥æ ¹æ®ä¸Šä¸‹æ–‡åˆ¤æ–­ï¼‰",
            "reason": "åœ¨æ­¤å¤„å¡«å†™é€‰æ‹©è¯¥å¥çš„æ‹çˆ±ç†ç”±"
        }
    ],
    "chapter_summary": "åœ¨æ­¤å¤„å¡«å†™æœ¬ç« çš„æ‹çˆ±ä¸»çº¿æ€»ç»“"
}
</JSON>
ç« èŠ‚å†…å®¹å¦‚ä¸‹ï¼š
"""

# ==============================================================================
# 2. å·¥å…·å‡½æ•°ä¸è£…é¥°å™¨
# ==============================================================================

class RateLimitError(Exception): pass
class APITimeoutError(Exception): pass
class SensitiveWordsError(Exception): pass

def confirm_action(prompt: str) -> bool:
    """å‘ç”¨æˆ·è¯·æ±‚ç¡®è®¤å±é™©æ“ä½œ"""
    if not sys.stdin.isatty():
        logger.warning(f"éäº¤äº’ç¯å¢ƒï¼Œè‡ªåŠ¨è·³è¿‡æ“ä½œ: {prompt}")
        return False
    
    try:
        response = input(f"âš ï¸  {prompt} (yes/no): ").lower().strip()
        if response in ['y', 'yes']:
            return True
        logger.info("æ“ä½œå·²è¢«ç”¨æˆ·å–æ¶ˆã€‚")
        return False
    except (EOFError, KeyboardInterrupt):
        logger.info("\næ“ä½œå·²è¢«ç”¨æˆ·ä¸­æ–­ã€‚")
        return False

def retry_on_failure(max_retries: int, delay: float, backoff: float):
    """é‡è¯•è£…é¥°å™¨"""
    def decorator(func):
        @wraps(func)
        def wrapper(*args, **kwargs):
            current_delay = delay
            self_obj = args[0] if args else None
            for attempt in range(max_retries + 1):
                try:
                    return func(*args, **kwargs)
                except KeyboardInterrupt:
                    logger.info("æ£€æµ‹åˆ°ä¸­æ–­ä¿¡å·ï¼Œç»ˆæ­¢é‡è¯•æµç¨‹")
                    raise
                except Exception as e:
                    if attempt == max_retries:
                        logger.error(f"å‡½æ•° {func.__name__} åœ¨ {max_retries} æ¬¡é‡è¯•åä»ç„¶å¤±è´¥: {e}")
                        raise
                    
                    logger.warning(f"å‡½æ•° {func.__name__} ç¬¬ {attempt + 1} æ¬¡å°è¯•å¤±è´¥: {e}")
                    logger.info(f"ç­‰å¾… {current_delay:.1f} ç§’åé‡è¯•...")
                    
                    interrupt_handler = getattr(self_obj, 'interrupt_handler', None)
                    if interrupt_handler:
                        interrupt_handler.safe_sleep(current_delay)
                    else:
                        time.sleep(current_delay)
                    current_delay *= backoff
            return None
        return wrapper
    return decorator

# ==============================================================================
# 3. æ ¸å¿ƒæ¨¡å—ç±»
# ==============================================================================

class InterruptHandler:
    """å¤„ç†Ctrl+Cç­‰ä¸­æ–­ä¿¡å·"""
    def __init__(self):
        self.interrupted = False
        signal.signal(signal.SIGINT, self._handle_interrupt)
        signal.signal(signal.SIGTERM, self._handle_interrupt)
    
    def _handle_interrupt(self, signum, frame):
        logger.info("æ”¶åˆ°ä¸­æ–­ä¿¡å·ï¼Œæ­£åœ¨å®‰å…¨é€€å‡º...")
        self.interrupted = True
    
    def should_continue(self) -> bool:
        return not self.interrupted

    def safe_sleep(self, seconds: float):
        """å¯ä¸­æ–­çš„sleep"""
        for _ in range(int(seconds)):
            if self.interrupted: raise KeyboardInterrupt
            time.sleep(1)
        if seconds % 1 > 0:
            if self.interrupted: raise KeyboardInterrupt
            time.sleep(seconds % 1)

class FileManager:
    """è´Ÿè´£æ‰€æœ‰æ–‡ä»¶è¯»å†™ã€åŠ è½½ã€ä¿å­˜å’Œæ¸…ç†æ“ä½œ"""
    def __init__(self, config: Config):
        self.config = config

    def clean_all(self):
        """æ¸…ç†æ‰€æœ‰ç”Ÿæˆçš„æ–‡ä»¶ï¼Œå¹¶è¯·æ±‚ç”¨æˆ·ç¡®è®¤"""
        logger.info("å¼€å§‹æ¸…ç†åˆ†ææ–‡ä»¶...")
        
        # æ¸…ç†è¿›åº¦æ–‡ä»¶
        progress_file = Path(self.config.progress_file)
        if progress_file.exists():
            if confirm_action(f"æ˜¯å¦åˆ é™¤è¿›åº¦æ–‡ä»¶ '{progress_file}'?"):
                progress_file.unlink()
                logger.info(f"è¿›åº¦æ–‡ä»¶å·²åˆ é™¤: {progress_file}")

        # æ¸…ç†ç»“æœæ–‡ä»¶ï¼ˆåŒ…æ‹¬åˆ†å—ï¼‰
        output_path = Path(self.config.output_path)
        if output_path.exists():
             if confirm_action(f"æ˜¯å¦åˆ é™¤ä¸»ç»“æœæ–‡ä»¶ '{output_path}'?"):
                output_path.unlink()
                logger.info(f"ä¸»ç»“æœæ–‡ä»¶å·²åˆ é™¤: {output_path}")
        
        chunk_files = list(output_path.parent.glob(f"{output_path.stem}_part_*{output_path.suffix}"))
        if chunk_files:
            if confirm_action(f"æ˜¯å¦åˆ é™¤ {len(chunk_files)} ä¸ªç»“æœåˆ†å—æ–‡ä»¶?"):
                for f in chunk_files:
                    f.unlink()
                logger.info(f"{len(chunk_files)} ä¸ªç»“æœåˆ†å—æ–‡ä»¶å·²åˆ é™¤ã€‚")

        # æ¸…ç†æŠ¥å‘Šæ–‡ä»¶
        report_path = Path(self.config.report_path)
        if report_path.exists():
            if confirm_action(f"æ˜¯å¦åˆ é™¤æŠ¥å‘Šæ–‡ä»¶ '{report_path}'?"):
                report_path.unlink()
                logger.info(f"æŠ¥å‘Šæ–‡ä»¶å·²åˆ é™¤: {report_path}")
        
        logger.info("æ–‡ä»¶æ¸…ç†å®Œæˆã€‚")

    def load_progress(self) -> Dict[str, Any]:
        """åŠ è½½è¿›åº¦ï¼Œå¹¶èƒ½è‡ªåŠ¨è¿ç§»æ—§ç‰ˆæœ¬è¿›åº¦æ–‡ä»¶"""
        new_progress_file = Path(self.config.progress_file)
        old_progress_file = Path("analysis_progress.pkl")

        # ä¼˜å…ˆä½¿ç”¨æ–°æ–‡ä»¶
        if new_progress_file.exists():
            try:
                with open(new_progress_file, 'rb') as f:
                    progress = pickle.load(f)
                logger.info(f"å·²åŠ è½½è¿›åº¦: {len(progress.get('completed_chapters', []))}/{progress.get('total_chapters', 0)} ç« èŠ‚å®Œæˆ")
                return progress
            except Exception as e:
                logger.error(f"åŠ è½½è¿›åº¦æ–‡ä»¶ '{new_progress_file}' å¤±è´¥: {e}")
        
        # å¦‚æœæ–°æ–‡ä»¶ä¸å­˜åœ¨ï¼Œåˆ™å°è¯•åŠ è½½å¹¶è¿ç§»æ—§æ–‡ä»¶
        elif old_progress_file.exists():
            logger.info(f"æœªæ‰¾åˆ°æ–°è¿›åº¦æ–‡ä»¶ï¼Œä½†æ£€æµ‹åˆ°æ—§è¿›åº¦æ–‡ä»¶ '{old_progress_file}'ã€‚å°†åŠ è½½å¹¶è¿ç§»ã€‚")
            try:
                with open(old_progress_file, 'rb') as f:
                    progress = pickle.load(f)
                logger.info(f"æˆåŠŸåŠ è½½æ—§è¿›åº¦ã€‚ä»»ä½•æ–°è¿›åº¦å°†è¢«ä¿å­˜åˆ° '{new_progress_file}'ã€‚")
                return progress
            except Exception as e:
                logger.error(f"åŠ è½½æ—§è¿›åº¦æ–‡ä»¶ '{old_progress_file}' å¤±è´¥: {e}")

        return self._get_default_progress()

    def save_progress(self, progress: Dict[str, Any]):
        """ä¿å­˜è¿›åº¦"""
        try:
            progress['last_save_time'] = datetime.now().isoformat()
            with open(self.config.progress_file, 'wb') as f:
                pickle.dump(progress, f)
            logger.debug("è¿›åº¦å·²ä¿å­˜")
        except Exception as e:
            logger.error(f"ä¿å­˜è¿›åº¦å¤±è´¥: {e}")

    def _get_default_progress(self) -> Dict[str, Any]:
        return {
            'completed_chapters': [], 'failed_chapters': [], 'current_chapter_index': 0,
            'total_chapters': 0, 'start_time': None, 'last_save_time': None
        }

    def load_existing_results(self) -> Optional[Dict]:
        """åŠ è½½ç»“æœæ–‡ä»¶ï¼Œå¹¶èƒ½è‡ªåŠ¨è¿ç§»æ—§ç‰ˆæœ¬ç»“æœ"""
        # ä¼˜å…ˆå°è¯•åŠ è½½æ–°ç‰ˆæœ¬æ–‡ä»¶
        results = self._load_results_from_path(self.config.output_path)
        if results:
            return results

        # å¦‚æœæ–°æ–‡ä»¶ä¸å­˜åœ¨ï¼Œå°è¯•åŠ è½½æ—§ç‰ˆæœ¬æ–‡ä»¶
        old_output_path = "analysis_results.json"
        logger.info(f"æœªæ‰¾åˆ°æ–°ç»“æœæ–‡ä»¶ï¼Œå°è¯•æŸ¥æ‰¾æ—§ç»“æœæ–‡ä»¶ '{old_output_path}'...")
        results = self._load_results_from_path(old_output_path)
        if results:
            logger.info(f"æˆåŠŸåŠ è½½æ—§ç»“æœã€‚ä»»ä½•æ–°ç»“æœå°†è¢«ä¿å­˜åˆ° '{self.config.output_path}' ç›¸å…³æ–‡ä»¶ä¸­ã€‚")
            return results
            
        return None

    def _load_results_from_path(self, path_str: str) -> Optional[Dict]:
        """ä»æŒ‡å®šè·¯å¾„åŠ è½½å•ä¸ªæˆ–åˆ†å—çš„ç»“æœæ–‡ä»¶"""
        output_path = Path(path_str)
        base_path, file_stem, file_suffix = output_path, output_path.stem, output_path.suffix
        
        # ç­›é€‰å‡ºç¬¦åˆå‘½åè§„åˆ™çš„æ–‡ä»¶å¹¶æ’åº
        valid_chunk_files = []
        for p in base_path.parent.glob(f"{file_stem}_part_*{file_suffix}"):
            match = re.search(r'_part_(\d+)', p.name)
            if match:
                valid_chunk_files.append((p, int(match.group(1))))
            else:
                logger.warning(f"å‘ç°ä¸ç¬¦åˆå‘½åè§„åˆ™çš„æ–‡ä»¶ï¼Œå·²è·³è¿‡: {p.name}")
        
        chunk_files = [p for p, num in sorted(valid_chunk_files, key=lambda item: item[1])]

        if not chunk_files:
            if base_path.exists():
                logger.info(f"æ‰¾åˆ°å•æ–‡ä»¶ç»“æœ: {base_path}")
                try:
                    with open(base_path, 'r', encoding='utf-8') as f: return json.load(f)
                except Exception as e: logger.error(f"åŠ è½½æ–‡ä»¶ '{base_path}' å¤±è´¥: {e}")
            return None

        logger.info(f"æ‰¾åˆ° {len(chunk_files)} ä¸ª '{file_stem}' ç³»åˆ—çš„åˆ†å—æ–‡ä»¶ï¼Œæ­£åœ¨åˆå¹¶...")
        merged_results, all_chapters = None, []
        for chunk_file in chunk_files:
            try:
                with open(chunk_file, 'r', encoding='utf-8') as f:
                    chunk_data = json.load(f)
                if merged_results is None:
                    merged_results = chunk_data
                all_chapters.extend(chunk_data.get('chapters', []))
            except json.JSONDecodeError:
                logger.warning(f"æ–‡ä»¶ {chunk_file} æ ¼å¼æŸåï¼Œå·²è·³è¿‡ã€‚")
            except Exception as e:
                logger.error(f"åŠ è½½åˆ†å—æ–‡ä»¶ {chunk_file} æ—¶å‘ç”ŸæœªçŸ¥é”™è¯¯: {e}")
        
        if merged_results:
            merged_results['chapters'] = all_chapters
            logger.info(f"æˆåŠŸåˆå¹¶ {len(all_chapters)} ä¸ªç« èŠ‚çš„ç»“æœ")
            return merged_results
        return None

    def save_results(self, results: Dict):
        """å°†åˆ†æç»“æœæŒ‰æŒ‡å®šå¤§å°åˆ†å—ä¿å­˜"""
        try:
            output_path = Path(self.config.output_path)
            base_path, file_stem, file_suffix = output_path, output_path.stem, output_path.suffix
            output_dir = base_path.parent
            chunk_size = self.config.chunk_size

            if not output_dir.exists(): output_dir.mkdir(parents=True)

            all_chapters = results.get('chapters', [])
            
            # æ ¸å¿ƒä¿®å¤ï¼šåœ¨ä¿å­˜å‰ï¼Œå§‹ç»ˆæŒ‰ç« èŠ‚ç´¢å¼•æ’åºï¼Œç¡®ä¿è¾“å‡ºæ–‡ä»¶é¡ºåºæ­£ç¡®
            all_chapters.sort(key=lambda c: c.get('chapter_index', 0))
            
            if not all_chapters:
                logger.info("æ²¡æœ‰ç« èŠ‚ç»“æœéœ€è¦ä¿å­˜")
                return

            num_chunks = (len(all_chapters) + chunk_size - 1) // chunk_size
            for i in range(num_chunks):
                chunk_path = output_dir / f"{file_stem}_part_{i+1}{file_suffix}"
                chunk_chapters = all_chapters[i*chunk_size : (i+1)*chunk_size]
                
                chunk_results = results.copy()
                chunk_results['chapters'] = chunk_chapters
                chunk_results['novel_info'] = results['novel_info'].copy()
                chunk_results['novel_info']['chunk_number'] = i + 1
                chunk_results['novel_info']['total_chunks'] = num_chunks

                with open(chunk_path, 'w', encoding='utf-8') as f:
                    json.dump(chunk_results, f, ensure_ascii=False, indent=2)
            
            logger.info(f"åˆ†æç»“æœå·²åˆ†å—ä¿å­˜åˆ° {num_chunks} ä¸ªæ–‡ä»¶ä¸­ã€‚")

            if base_path.exists():
                if confirm_action(f"æ˜¯å¦åˆ é™¤æ—§çš„ã€æœªåˆ†å—çš„ç»“æœæ–‡ä»¶ '{base_path}'?"):
                    base_path.unlink()
                    logger.info(f"å·²åˆ é™¤æ—§çš„å•ä½“ç»“æœæ–‡ä»¶: {base_path}")

        except Exception as e:
            logger.error(f"ä¿å­˜åˆ†å—ç»“æœå¤±è´¥: {e}\n{traceback.format_exc()}")
            raise

    def generate_report(self, results: Dict):
        """ç”ŸæˆMarkdownæ ¼å¼çš„åˆ†ææŠ¥å‘Š"""
        report_path = self.config.report_path
        logger.info(f"æ­£åœ¨ç”Ÿæˆåˆ†ææŠ¥å‘Š: {report_path}")
        
        with open(report_path, 'w', encoding='utf-8') as f:
            info = results['novel_info']
            f.write(f"# å°è¯´é‡‘å¥åˆ†ææŠ¥å‘Š: {Path(info['file_path']).name}\n\n")
            f.write("## æ‘˜è¦\n")
            f.write(f"- **æ€»ç« èŠ‚æ•°**: {info['total_chapters']}\n")
            f.write(f"- **å·²å®Œæˆ**: {info.get('completed_chapters', 0)}\n")
            f.write(f"- **æ€»å­—æ•°**: {info['total_words']:,}\n")
            f.write(f"- **æ€»é‡‘å¥æ•°**: {info['total_golden_sentences']}\n")
            f.write(f"- **åˆ†æç”¨æ—¶**: {info.get('analysis_duration', 'N/A')}\n\n")
            
            f.write("---\n\n")
            
            for chapter in sorted(results['chapters'], key=lambda c: c['chapter_index']):
                f.write(f"### ç« èŠ‚ {chapter['chapter_index']}: {chapter['chapter_title']}\n")
                f.write(f"å­—æ•°: {chapter['word_count']} | é‡‘å¥æ•°: {len(chapter.get('golden_sentences', []))}\n\n")
                if chapter.get('chapter_summary'):
                    f.write(f"**æ€»ç»“**: {chapter['chapter_summary']}\n\n")
                if chapter.get('golden_sentences'):
                    f.write("**é‡‘å¥åˆ—è¡¨**:\n")
                    for i, s in enumerate(chapter['golden_sentences'], 1):
                        f.write(f"{i}. **{s.get('speaker', 'æœªçŸ¥')}**: {s.get('sentence', '')}\n")
                        f.write(f"   - *ç†ç”±*: {s.get('reason', 'AIåˆ†æ')}\n")
                    f.write("\n")
                f.write("---\n\n")
        
        logger.info(f"æŠ¥å‘Šå·²ç”Ÿæˆ: {report_path}")

class APIHandler:
    """å¤„ç†å¯¹AIæ¨¡å‹çš„APIè°ƒç”¨"""
    def __init__(self, config: Config, interrupt_handler: InterruptHandler):
        self.config = config
        self.interrupt_handler = interrupt_handler
        self.headers = {
            'Authorization': f'Bearer {config.api_key}',
            'Content-Type': 'application/json'
        }

    def analyze_content(self, content: str) -> Optional[Dict]:
        """è°ƒç”¨APIåˆ†æå†…å®¹ï¼Œæ ¹æ®é…ç½®é€‰æ‹©æ˜¯å¦ä½¿ç”¨JSON Schema"""
        
        system_prompt = (
            "ä½ æ˜¯ä¸€ä¸ªä¸“é—¨ç”¨äºæ–‡æœ¬åˆ†æçš„APIç«¯ç‚¹ã€‚ä½ çš„å”¯ä¸€ä»»åŠ¡æ˜¯æ ¹æ®ç”¨æˆ·æä¾›çš„å†…å®¹ï¼Œ"
            "è¿”å›ä¸€ä¸ªä¸¥æ ¼ç¬¦åˆJSONæ ¼å¼çš„å­—ç¬¦ä¸²ã€‚ä¸è¦æ·»åŠ ä»»ä½•è§£é‡Šã€æ³¨é‡Šæˆ–Markdownæ ‡è®°ã€‚"
            "ä½ çš„è¾“å‡ºå¿…é¡»èƒ½å¤Ÿç›´æ¥è¢«json.loads()è§£æã€‚"
        )

        api_params = {
            "temperature": 0.1,
            "max_tokens": self.config.max_tokens,
        }

        if self.config.use_json_schema:
            logger.info("æ­£åœ¨è°ƒç”¨APIå¹¶å¯ç”¨JSON Schemaæ¨¡å¼...")
            # åœ¨Schemaæ¨¡å¼ä¸‹ï¼Œä½¿ç”¨ä¸€ä¸ªæ›´ç®€æ´çš„ã€ä¸åŒ…å«æ ¼å¼æè¿°çš„æç¤ºè¯
            schema_prompt = """
ä½ ç°åœ¨éœ€è¦åˆ†æè¿™ç¯‡å†…å®¹å¹¶å®Œæˆä¸‹é¢çš„ä»»åŠ¡ï¼Œä½ éœ€è¦é€‰å–äººç”Ÿæ„Ÿæ‚Ÿæˆ–è€…æ˜¯æ‹çˆ±æ–¹æ³•ç»éªŒç­‰ï¼Œä¿ç•™ä¸Šä¸‹æ–‡ã€‚å¦‚æœæ²¡æœ‰å¯ä»¥å†é‡‘å¥é‚£é‡Œè®¾ç½®ä¸€ä¸ªç©ºåˆ—è¡¨
è¯·æ³¨æ„ï¼š
1.è¯·å‹¿åŒ…å«æ•…äº‹æƒ…èŠ‚ï¼šæ ·ä¾‹ï¼š
1>è™½ç„¶ä»–ä¸çŸ¥é“åˆ«çš„å°è¯´ä½œè€…æ˜¯æ€ä¹ˆæ ·çš„ï¼Œä½†ä»–å¹³æ—¶å°±å–œæ¬¢å»æ›´å¤šçš„åœ°æ–¹ï¼Œè§æ›´å¤šçš„é£æ™¯ï¼Œè¿™æ ·æ‰èƒ½å†™å‡ºæ¥æ›´åŠ ç²¾å½©çš„æ•…äº‹ã€‚
2>ä¸çŸ¥ä¸è§‰ä¸­ï¼Œè‹ç™½ç²¥çš„å¿ƒæƒ…å˜å¾—ååˆ†å®‰é€¸ï¼Œä¹‹å‰çš„ä¸æ„‰å¿«å·²ç»çƒŸæ¶ˆäº‘æ•£ã€‚
2.ä½ çš„ç›®çš„æ˜¯ä¿å­˜é‡‘å¥è®©äººæ„Ÿè§‰æœ‰ä»·å€¼çš„ï¼Œä¸æ˜¯ä¿ç•™äººç‰©çš„æ— ç”¨å¯¹è¯ç­‰
ç« èŠ‚å†…å®¹å¦‚ä¸‹ï¼š
"""
            full_prompt = schema_prompt + content
            api_params["response_format"] = {
                "type": "json_schema",
                "json_schema": {
                    "name": "novel_analysis_response",
                    "schema": {
                        "type": "object",
                        "properties": {
                            "chapter_title": {"type": "string"},
                            "golden_sentences": {
                                "type": "array",
                                "items": {
                                    "type": "object",
                                    "properties": {
                                        "sentence": {"type": "string"},
                                        "speaker": {"type": "string"},
                                        "reason": {"type": "string"}
                                    },
                                    "required": ["sentence", "speaker", "reason"]
                                }
                            },
                            "chapter_summary": {"type": "string"}
                        },
                        "required": ["chapter_title", "golden_sentences", "chapter_summary"]
                    },
                    "strict": True,
                }
            }
        else:
            logger.info("æ­£åœ¨è°ƒç”¨APIå¹¶å¯ç”¨æ ‡å‡†JSONæ¨¡å¼...")
            full_prompt = self.config.prompt + content
            api_params["response_format"] = {"type": "json_object"}

        try:
            response = self._call_api(
                messages=[
                    {"role": "system", "content": system_prompt},
                    {"role": "user", "content": full_prompt}
                ],
                **api_params
            )
            
            if not response.get('choices') or not response['choices'][0].get('message', {}).get('content'):
                raise ValueError("APIè¿”å›ç©ºå“åº”")

            result_text = response['choices'][0]['message']['content']
            
            if response['choices'][0].get('finish_reason') == "length":
                logger.warning("APIå“åº”å› è¾¾åˆ°max_tokensè€Œè¢«æˆªæ–­ã€‚å»ºè®®å¢åŠ  --max-tokens çš„å€¼ã€‚")

            parsed_json = self._parse_api_response(result_text)
            if not isinstance(parsed_json, dict):
                 raise TypeError(f"è§£æåçš„JSONä¸æ˜¯ä¸€ä¸ªå­—å…¸ï¼Œè€Œæ˜¯ {type(parsed_json)}")
            return parsed_json

        except SensitiveWordsError as e:
            logger.error(f"ç« èŠ‚å†…å®¹åŒ…å«æ•æ„Ÿè¯ï¼Œå·²è¢«APIæ‹’ç»: {e}")
            raise
        except Exception as e:
            logger.error(f"APIè°ƒç”¨æˆ–è§£æå¤±è´¥: {e}\n{traceback.format_exc()}")
            raise

    @retry_on_failure(max_retries=5, delay=3.0, backoff=1.5)
    def _call_api(self, messages: List[Dict], **kwargs) -> Dict:
        """ä½¿ç”¨requestsåº“è¿›è¡ŒAPIè°ƒç”¨"""
        url = f"{self.config.base_url.rstrip('/')}/chat/completions"
        data = {"model": self.config.model, "messages": messages, **kwargs}

        response = requests.post(url, headers=self.headers, json=data, timeout=self.config.api_timeout)

        if response.status_code == 200: return response.json()
        if response.status_code == 429: raise RateLimitError(f"API rate limit: {response.text}")
        if response.status_code in [408, 504]: raise APITimeoutError(f"API timeout: {response.text}")
        
        # ä¸“é—¨å¤„ç†æ•æ„Ÿè¯é”™è¯¯
        if response.status_code == 400 and 'sensitive_words_detected' in response.text:
            raise SensitiveWordsError(f"APIæ£€æµ‹åˆ°æ•æ„Ÿè¯: {response.text}")
            
        raise Exception(f"API error {response.status_code}: {response.text}")

    def _parse_api_response(self, data: str) -> Optional[Dict]:
        """è§£æAPIå“åº”ï¼Œç”±äºä½¿ç”¨JSONæ¨¡å¼ï¼Œæ­¤å¤„å˜å¾—æ›´ç®€å•"""
        if isinstance(data, dict):
            return data # å¦‚æœå·²ç»æ˜¯å­—å…¸ï¼Œç›´æ¥è¿”å›
        try:
            # JSONæ¨¡å¼ä¸‹ï¼Œå“åº”æœ¬èº«å°±åº”è¯¥æ˜¯åˆæ³•çš„JSONå­—ç¬¦ä¸²
            return json.loads(data)
        except (json.JSONDecodeError, TypeError) as e:
            logger.error(f"å³ä½¿åœ¨JSONæ¨¡å¼ä¸‹ï¼Œè§£æä»ç„¶å¤±è´¥: {e}")
            logger.error(f"æ”¶åˆ°çš„æ— æ•ˆJSONæ•°æ®: {data}")
            return None

    def test_connection(self) -> bool:
        """æµ‹è¯•APIè¿æ¥å’Œè®¤è¯"""
        logger.info("æ­£åœ¨æµ‹è¯•APIè¿æ¥...")
        try:
            self._call_api(
                messages=[{"role": "user", "content": "Say 'test'"}],
                max_tokens=5,
                temperature=0
            )
            logger.info("âœ… APIè¿æ¥æˆåŠŸï¼")
            return True
        except Exception as e:
            logger.error(f"âŒ APIè¿æ¥å¤±è´¥: {e}")
            return False

class EPUBParser:
    """è´Ÿè´£è§£æEPUBæ–‡ä»¶å’Œæå–ç« èŠ‚"""
    def extract_chapters(self, epub_path: str) -> List[Dict]:
        logger.info(f"å¼€å§‹æå–EPUBæ–‡ä»¶: {epub_path}")
        try:
            book = epub.read_epub(epub_path)
            chapters = []
            for item in book.get_items_of_type(ebooklib.ITEM_DOCUMENT):
                soup = BeautifulSoup(item.get_content(), 'html.parser')
                text = re.sub(r'\s+', ' ', soup.get_text()).strip()
                if len(text) > 100:
                    title = self._extract_title(item.get_name(), text)
                    chapters.append({'title': title, 'content': text, 'word_count': len(text)})
            logger.info(f"æˆåŠŸæå– {len(chapters)} ä¸ªç« èŠ‚")
            return chapters
        except Exception as e:
            logger.error(f"æå–EPUBæ–‡ä»¶å¤±è´¥: {e}\n{traceback.format_exc()}")
            raise

    def _extract_title(self, filename: str, content: str) -> str:
        # ä¼˜å…ˆä»å†…å®¹å‰å‡ è¡Œæå–
        for line in content.split('\n')[:5]:
            line = line.strip()
            if line and (('ç¬¬' in line and 'ç« ' in line) or len(line) < 50):
                return line
        return Path(filename).stem

# ==============================================================================
# 4. ä¸»ç¼–æ’å™¨
# ==============================================================================

class AnalysisOrchestrator:
    """ä¸»ç¼–æ’å™¨ï¼Œåè°ƒæ‰€æœ‰æ¨¡å—å®Œæˆåˆ†æä»»åŠ¡"""
    def __init__(self, config: Config):
        self.config = config
        self.interrupt_handler = InterruptHandler()
        self.file_manager = FileManager(config)
        self.api_handler = APIHandler(config, self.interrupt_handler)
        self.epub_parser = EPUBParser()
        
        self.progress = self.file_manager.load_progress()

    def run_analysis(self, resume: bool = True, test_mode: bool = False, re_analyze_chapters: Optional[List[int]] = None, retry_failed: bool = False, find_missing: bool = False):
        """æ‰§è¡Œå®Œæ•´çš„å°è¯´åˆ†ææµç¨‹"""
        if find_missing:
            self.run_find_missing_chapters()
            return

        if re_analyze_chapters:
            self._prepare_for_reanalysis(re_analyze_chapters)
        
        if retry_failed:
            self.run_retry_failed_chapters()
            return

        start_time = datetime.now()
        logger.info(f"å¼€å§‹åˆ†æå°è¯´: {self.config.epub_path}")

        try:
            chapters = self.epub_parser.extract_chapters(self.config.epub_path)
            if not chapters:
                logger.error("æœªèƒ½æå–ä»»ä½•ç« èŠ‚ï¼Œç¨‹åºç»ˆæ­¢ã€‚")
                return

            if test_mode:
                logger.info("æµ‹è¯•æ¨¡å¼å¯åŠ¨ï¼Œä»…åˆ†æå‰3ç« ã€‚")
                chapters = chapters[:3]

            results = self._initialize_results(chapters, start_time, resume)
            
            # æ ¸å¿ƒä¿®å¤ï¼šå¦‚æœæŒ‡å®šäº†é‡æ–°åˆ†æï¼Œå¿…é¡»ä»å¤´æ‰«æï¼Œè€Œä¸æ˜¯ä»ä¸Šæ¬¡çš„ç´¢å¼•ç»§ç»­
            if re_analyze_chapters:
                start_index = 0
                logger.info("é‡æ–°åˆ†ææ¨¡å¼å·²æ¿€æ´»ï¼Œå°†ä»å¤´æ‰«ææ‰€æœ‰ç« èŠ‚ä»¥æ‰¾åˆ°å¾…å¤„ç†é¡¹ã€‚")
            else:
                start_index = self.progress.get('current_chapter_index', 0)
                logger.info(f"å°†ä»ç¬¬ {start_index} ç« å¼€å§‹åˆ†æã€‚")

            for i in range(start_index, len(chapters)):
                chapter = chapters[i]
                if self.interrupt_handler.interrupted:
                    logger.info("æ£€æµ‹åˆ°ä¸­æ–­ï¼Œæ­£åœ¨ä¿å­˜è¿›åº¦...")
                    break
                
                if self.is_chapter_completed(i):
                    continue

                progress_percent = (i + 1) / len(chapters) * 100
                logger.info(f"åˆ†æä¸­... {progress_percent:.1f}% ({i+1}/{len(chapters)})")
                try:
                    analysis_result = self.analyze_single_chapter(chapter, i)
                    if analysis_result:
                        self._update_chapter_in_results(results, analysis_result)
                        self.mark_chapter_completed(i)
                        logger.info(self._get_concise_result_log(analysis_result))
                    else:
                        self.mark_chapter_failed(i, "åˆ†æè¿”å›ç©ºç»“æœ")
                except Exception as e:
                    logger.error(f"å¤„ç†ç« èŠ‚ {i} æ—¶å‘ç”Ÿä¸¥é‡é”™è¯¯: {e}") # ç§»é™¤tracebackä»¥ç®€åŒ–æ—¥å¿—
                    self.mark_chapter_failed(i, str(e))
                
                if (i + 1) % 5 == 0:
                    self.file_manager.save_results(results)
                
                self.interrupt_handler.safe_sleep(self.config.rate_limit_delay)

            self._finalize_analysis(results, start_time)
            self.file_manager.save_results(results)
            self.file_manager.generate_report(results)

        except KeyboardInterrupt:
            logger.warning("\nç¨‹åºè¢«ç”¨æˆ·ä¸­æ–­ã€‚è¿›åº¦å·²ä¿å­˜ã€‚")
        except Exception as e:
            logger.error(f"åˆ†æè¿‡ç¨‹ä¸­å‘ç”Ÿè‡´å‘½é”™è¯¯: {e}\n{traceback.format_exc()}")
        finally:
            if 'results' in locals() and results.get('chapters'):
                self.file_manager.save_results(results)
                logger.info("æœ€ç»ˆç»“æœå·²ä¿å­˜ã€‚")

    def analyze_single_chapter(self, chapter: Dict, index: int) -> Optional[Dict]:
        """åˆ†æå•ä¸ªç« èŠ‚ï¼Œå¹¶å¤„ç†ç‰¹å®šAPIé”™è¯¯"""
        content = chapter['content']
        if len(content) > self.config.long_chapter_threshold:
            logger.info(f"ç« èŠ‚ {index} å†…å®¹è¿‡é•¿ï¼Œå°†è¿›è¡Œæˆªæ–­å¤„ç†ä»¥é¿å…APIé”™è¯¯ã€‚")
            content = content[:self.config.long_chapter_threshold]

        try:
            api_result = self.api_handler.analyze_content(content)
            if not api_result:
                return None

            # è¡¥å……å…ƒæ•°æ®
            api_result['original_title'] = chapter['title']
            api_result['word_count'] = chapter['word_count']
            api_result['chapter_index'] = index
            api_result['analysis_timestamp'] = datetime.now().isoformat()
            return api_result
        except SensitiveWordsError as e:
            # æ•è·æ•æ„Ÿè¯é”™è¯¯ï¼Œæ ‡è®°å¤±è´¥å¹¶ç»§ç»­
            logger.error(f"ç« èŠ‚ {index} ({chapter['title']}) å› åŒ…å«æ•æ„Ÿè¯è€Œæ— æ³•åˆ†æã€‚")
            self.mark_chapter_failed(index, f"å†…å®¹åŒ…å«æ•æ„Ÿè¯ï¼Œè¢«APIæ‹’ç»: {e}")
            return None

    def _initialize_results(self, chapters: List[Dict], start_time: datetime, resume: bool) -> Dict:
        """åˆå§‹åŒ–æˆ–åŠ è½½åˆ†æç»“æœ"""
        if resume:
            results = self.file_manager.load_existing_results()
            if results:
                logger.info(f"ä»ä¸Šæ¬¡ä¸­æ–­å¤„æ¢å¤åˆ†æï¼Œå·²å®Œæˆ {len(results['chapters'])} ç« ")
                results['novel_info']['resume_count'] = results['novel_info'].get('resume_count', 0) + 1
                results['novel_info']['last_resume_time'] = start_time.isoformat()
                return results
        
        logger.info("æœªæ‰¾åˆ°æˆ–ä¸ä½¿ç”¨æ¢å¤æ•°æ®ï¼Œå¼€å§‹æ–°çš„åˆ†æã€‚")
        self.progress = self.file_manager._get_default_progress() # é‡ç½®è¿›åº¦
        return {
            'novel_info': {
                'file_path': self.config.epub_path,
                'total_chapters': len(chapters),
                'total_words': sum(c['word_count'] for c in chapters),
                'analysis_start_time': start_time.isoformat(),
                'resume_count': 0
            },
            'chapters': []
        }

    def _finalize_analysis(self, results: Dict, start_time: datetime):
        """å®Œæˆåˆ†æå¹¶æ›´æ–°æœ€ç»ˆç»Ÿè®¡ä¿¡æ¯"""
        end_time = datetime.now()
        completed_chapters = len(results['chapters'])
        total_sentences = sum(len(ch.get('golden_sentences', [])) for ch in results['chapters'])
        
        results['novel_info']['analysis_end_time'] = end_time.isoformat()
        results['novel_info']['analysis_duration'] = str(end_time - start_time)
        results['novel_info']['completed_chapters'] = completed_chapters
        results['novel_info']['total_golden_sentences'] = total_sentences
        
        logger.info(f"åˆ†æå®Œæˆ! æ€»ç”¨æ—¶: {end_time - start_time}")
        logger.info(f"å®Œæˆç« èŠ‚: {completed_chapters}/{results['novel_info']['total_chapters']}")
        logger.info(f"å…±æ‰¾åˆ° {total_sentences} ä¸ªé‡‘å¥")

    def _prepare_for_reanalysis(self, chapters_to_reanalyze: List[int]):
        """ä¸ºé‡æ–°åˆ†æå‡†å¤‡ï¼Œä»è¿›åº¦ä¸­ç§»é™¤æŒ‡å®šç« èŠ‚"""
        logger.info(f"å‡†å¤‡é‡æ–°åˆ†æç« èŠ‚: {chapters_to_reanalyze}")
        
        original_completed = set(self.progress['completed_chapters'])
        chapters_to_remove = set(chapters_to_reanalyze)
        
        self.progress['completed_chapters'] = list(original_completed - chapters_to_remove)
        
        # å¯é€‰ï¼šä¹Ÿä»å¤±è´¥åˆ—è¡¨ä¸­ç§»é™¤ï¼Œä»¥ä¾¿é‡è¯•
        self.progress['failed_chapters'] = [
            f for f in self.progress.get('failed_chapters', []) if f['index'] not in chapters_to_remove
        ]
        
        self.file_manager.save_progress(self.progress)
        logger.info("è¿›åº¦å·²æ›´æ–°ï¼Œç°åœ¨å°†é‡æ–°åˆ†ææŒ‡å®šç« èŠ‚ã€‚")

    def _get_concise_result_log(self, result: Dict) -> str:
        """ä¸ºæ—¥å¿—ç”Ÿæˆä¸€ä¸ªç®€æ˜æ‰¼è¦çš„ç»“æœæ‘˜è¦"""
        num_sentences = len(result.get('golden_sentences', []))
        summary = result.get('chapter_summary', 'æ— æ‘˜è¦')
        truncated_summary = (summary[:40] + '...') if len(summary) > 40 else summary
        return f"-> æ‰¾åˆ° {num_sentences} ä¸ªé‡‘å¥ã€‚æ‘˜è¦: '{truncated_summary}'"

    def run_retry_failed_chapters(self):
        """ä»…é‡è¯•æ‰€æœ‰å¤±è´¥çš„ç« èŠ‚"""
        logger.info("å¼€å§‹é‡è¯•æ‰€æœ‰å¤±è´¥çš„ç« èŠ‚...")
        results = self.file_manager.load_existing_results()
        if not results:
            logger.error("æœªæ‰¾åˆ°ç»“æœæ–‡ä»¶ï¼Œæ— æ³•è¿›è¡Œé‡è¯•ã€‚è¯·å…ˆæ­£å¸¸è¿è¡Œä¸€æ¬¡ã€‚")
            return

        chapters = self.epub_parser.extract_chapters(self.config.epub_path)
        
        all_failed_chapters = self.progress.get('failed_chapters', [])
        completed_chapters_set = set(self.progress.get('completed_chapters', []))

        # æ ¸å¿ƒä¿®å¤ï¼šåªé‡è¯•é‚£äº›åœ¨å¤±è´¥åˆ—è¡¨é‡Œï¼Œä½†ä¸åœ¨æˆåŠŸåˆ—è¡¨é‡Œçš„ç« èŠ‚
        chapters_to_retry = [
            f for f in all_failed_chapters if f['index'] not in completed_chapters_set
        ]

        if not chapters_to_retry:
            logger.info("æ²¡æœ‰çœŸæ­£éœ€è¦é‡è¯•çš„å¤±è´¥ç« èŠ‚ã€‚ï¼ˆå¤±è´¥åˆ—è¡¨å¯èƒ½åŒ…å«å·²åœ¨åç»­è¿è¡Œä¸­æˆåŠŸçš„ç« èŠ‚ï¼‰")
            # å¯é€‰ï¼šæ¸…ç†è¿‡æ—¶çš„å¤±è´¥åˆ—è¡¨
            if len(all_failed_chapters) > 0:
                self.progress['failed_chapters'] = []
                self.file_manager.save_progress(self.progress)
                logger.info("å·²æ¸…ç†è¿‡æ—¶çš„å¤±è´¥ç« èŠ‚åˆ—è¡¨ã€‚")
            return

        logger.info(f"å°†å°è¯•é‡è¯• {len(chapters_to_retry)} ä¸ªçœŸæ­£å¤±è´¥çš„ç« èŠ‚ã€‚")
        
        for failed_info_index, failed_info in enumerate(chapters_to_retry):
            if self.interrupt_handler.interrupted:
                logger.info("æ£€æµ‹åˆ°ä¸­æ–­ï¼Œåœæ­¢é‡è¯•æµç¨‹ã€‚")
                break
            
            index = failed_info['index']
            chapter = chapters[index]
            logger.info(f"é‡è¯•ç« èŠ‚ {index}: {chapter['title']}")
            
            try:
                analysis_result = self.analyze_single_chapter(chapter, index)
                if analysis_result:
                    self._update_chapter_in_results(results, analysis_result)
                    self.mark_chapter_completed(index)
                    # ä»å¤±è´¥åˆ—è¡¨ä¸­ç§»é™¤
                    self.progress['failed_chapters'] = [f for f in self.progress['failed_chapters'] if f['index'] != index]
                    logger.info(f"ç« èŠ‚ {index} é‡è¯•æˆåŠŸã€‚")
                else:
                    logger.error(f"ç« èŠ‚ {index} é‡è¯•åä»ç„¶å¤±è´¥ã€‚")
            except Exception as e:
                logger.error(f"é‡è¯•ç« èŠ‚ {index} æ—¶å‘ç”Ÿä¸¥é‡é”™è¯¯: {e}")

            # åœ¨é‡è¯•æ¨¡å¼ä¸‹ä¹Ÿå®šæœŸä¿å­˜
            if (failed_info_index + 1) % 5 == 0:
                self.file_manager.save_results(results)
                logger.info(f"å·²ä¿å­˜é‡è¯•è¿›åº¦ (å¤„ç†åˆ°ç¬¬ {failed_info_index + 1}/{len(chapters_to_retry)} ä¸ªå¤±è´¥ç« èŠ‚)")

            self.interrupt_handler.safe_sleep(self.config.rate_limit_delay)

        self.file_manager.save_results(results)
        self.file_manager.save_progress(self.progress)
        logger.info("å¤±è´¥ç« èŠ‚é‡è¯•å®Œæˆã€‚")

    def run_find_missing_chapters(self):
        """
        æ ¡éªŒæ¨¡å¼ï¼šæ‰«æç»“æœæ–‡ä»¶ï¼Œæ‰¾å‡ºæ‰€æœ‰ç¼ºå¤±çš„ç« èŠ‚å¹¶è¿›è¡Œåˆ†æã€‚
        è¿™æ˜¯æœ€å¯é çš„ç»­è·‘æ¨¡å¼ã€‚
        """
        logger.info("--- Running in Find Missing Chapters Mode ---")
        
        # 1. è·å–æ‰€æœ‰ç« èŠ‚çš„å…ƒä¿¡æ¯
        chapters = self.epub_parser.extract_chapters(self.config.epub_path)
        if not chapters:
            logger.error("æœªèƒ½ä»EPUBä¸­æå–ä»»ä½•ç« èŠ‚ï¼Œæ— æ³•ç»§ç»­ã€‚")
            return

        # 2. åŠ è½½æ‰€æœ‰å·²å­˜åœ¨çš„ç»“æœ
        results = self.file_manager.load_existing_results()
        if not results:
            logger.info("æœªæ‰¾åˆ°ä»»ä½•ç°æœ‰ç»“æœï¼Œå°†åˆ†ææ‰€æœ‰ç« èŠ‚ã€‚")
            results = self._initialize_results(chapters, datetime.now(), resume=False)
        else:
            logger.info(f"å·²åŠ è½½ {len(results.get('chapters', []))} ä¸ªç°æœ‰ç»“æœã€‚")

        # 3. è¯†åˆ«ç¼ºå¤±çš„ç« èŠ‚
        completed_indices = {ch['chapter_index'] for ch in results.get('chapters', [])}
        all_indices = set(range(len(chapters)))
        missing_indices = sorted(list(all_indices - completed_indices))

        if not missing_indices:
            logger.info("ğŸ‰ æ‰€æœ‰ç« èŠ‚å‡å·²å®Œæˆåˆ†æå¹¶å­˜åœ¨äºç»“æœæ–‡ä»¶ä¸­ã€‚æ— éœ€æ“ä½œã€‚")
            return

        logger.warning(f"æ ¡éªŒå‘ç° {len(missing_indices)} ä¸ªç¼ºå¤±çš„ç« èŠ‚ã€‚å³å°†å¼€å§‹åˆ†æ...")
        self.interrupt_handler.safe_sleep(2)

        # 4. åˆ†æç¼ºå¤±çš„ç« èŠ‚
        for i, index in enumerate(missing_indices):
            if self.interrupt_handler.interrupted:
                logger.info("æ£€æµ‹åˆ°ä¸­æ–­ï¼Œåœæ­¢åˆ†æã€‚")
                break

            chapter_info = chapters[index]
            logger.info(f"æ­£åœ¨åˆ†æç¼ºå¤±ç« èŠ‚ {index} (æ€»è¿›åº¦ {i+1}/{len(missing_indices)}): '{chapter_info['title']}'")

            try:
                analysis_result = self.analyze_single_chapter(chapter_info, index)
                if analysis_result:
                    self._update_chapter_in_results(results, analysis_result)
                    self.mark_chapter_completed(index)
                    logger.info(self._get_concise_result_log(analysis_result))
                else:
                    self.mark_chapter_failed(index, "åœ¨æ ¡éªŒæ¨¡å¼ä¸‹åˆ†æè¿”å›ç©ºç»“æœ")
            except Exception as e:
                logger.error(f"å¤„ç†ç¼ºå¤±ç« èŠ‚ {index} æ—¶å‘ç”Ÿä¸¥é‡é”™è¯¯: {e}")
                self.mark_chapter_failed(index, f"æ ¡éªŒæ¨¡å¼ä¸‹å‘ç”Ÿå¼‚å¸¸: {str(e)}")

            if (i + 1) % 5 == 0:
                self.file_manager.save_results(results)
                logger.info(f"å·²ä¿å­˜æ ¡éªŒè¿›åº¦ (å·²å¤„ç† {i + 1}/{len(missing_indices)} ä¸ªç¼ºå¤±ç« èŠ‚)ã€‚")

            self.interrupt_handler.safe_sleep(self.config.rate_limit_delay)

        # 5. æœ€ç»ˆä¿å­˜
        logger.info("ç¼ºå¤±ç« èŠ‚å¤„ç†å®Œæ¯•ã€‚æ­£åœ¨æ‰§è¡Œæœ€ç»ˆä¿å­˜...")
        start_time_str = results.get('novel_info', {}).get('analysis_start_time')
        start_time = datetime.fromisoformat(start_time_str) if start_time_str else datetime.now()
        self._finalize_analysis(results, start_time)
        self.file_manager.save_results(results)
        self.file_manager.generate_report(results)
        logger.info("æ‰€æœ‰ç¼ºå¤±ç« èŠ‚å‡å·²å¤„ç†å®Œæ¯•ï¼Œç»“æœå’ŒæŠ¥å‘Šå·²æ›´æ–°ã€‚")

    def _update_chapter_in_results(self, results: Dict, new_chapter: Dict):
        """åœ¨ç»“æœåˆ—è¡¨ä¸­æ›´æ–°æˆ–æ·»åŠ ä¸€ä¸ªç« èŠ‚ï¼Œç¡®ä¿å”¯ä¸€æ€§ã€‚"""
        chapter_index = new_chapter['chapter_index']
        
        # æŸ¥æ‰¾ç°æœ‰ç« èŠ‚å¹¶æ›¿æ¢
        for i, chapter in enumerate(results['chapters']):
            if chapter.get('chapter_index') == chapter_index:
                logger.debug(f"æ‰¾åˆ°å¹¶æ›¿æ¢å·²å­˜åœ¨çš„ç« èŠ‚ {chapter_index} çš„ç»“æœã€‚")
                results['chapters'][i] = new_chapter
                return
        
        # å¦‚æœæœªæ‰¾åˆ°ï¼Œåˆ™è¿½åŠ 
        logger.debug(f"æœªæ‰¾åˆ°ç« èŠ‚ {chapter_index} çš„ç°æœ‰ç»“æœï¼Œå°†è¿½åŠ æ–°ç»“æœã€‚")
        results['chapters'].append(new_chapter)

    def is_chapter_completed(self, index: int) -> bool:
        """æ£€æŸ¥ç« èŠ‚æ˜¯å¦å·²å®Œæˆï¼ˆåŒæ—¶æ£€æŸ¥è¿›åº¦å’Œç°æœ‰ç»“æœï¼‰"""
        if index in self.progress['completed_chapters']:
            return True
        # å¯ä»¥åœ¨è¿™é‡ŒåŠ å…¥å¯¹ results çš„æ£€æŸ¥ï¼Œä½†è¿›åº¦æ–‡ä»¶åº”ä¸ºå‡†
        return False

    def mark_chapter_completed(self, index: int):
        """æ ‡è®°ç« èŠ‚å®Œæˆï¼Œå¹¶ç¡®ä¿å°†å…¶ä»å¤±è´¥åˆ—è¡¨ä¸­ç§»é™¤ã€‚"""
        if index not in self.progress['completed_chapters']:
            self.progress['completed_chapters'].append(index)
        
        # æ— è®ºå¦‚ä½•ï¼Œéƒ½å°è¯•ä»å¤±è´¥åˆ—è¡¨ä¸­ç§»é™¤
        original_failed_count = len(self.progress.get('failed_chapters', []))
        self.progress['failed_chapters'] = [
            f for f in self.progress.get('failed_chapters', []) if f['index'] != index
        ]
        if len(self.progress.get('failed_chapters', [])) < original_failed_count:
            logger.info(f"ç« èŠ‚ {index} å·²ä»å¤±è´¥åˆ—è¡¨ä¸­ç§»é™¤ã€‚")

        self.progress['current_chapter_index'] = max(self.progress.get('current_chapter_index', 0), index + 1)
        self.file_manager.save_progress(self.progress)

    def mark_chapter_failed(self, index: int, error: str):
        self.progress['failed_chapters'].append({'index': index, 'error': error})
        self.file_manager.save_progress(self.progress)

    def run_deduplicate_results(self):
        """åŠ è½½ã€å»é‡å¹¶é‡æ–°ä¿å­˜æ‰€æœ‰ç»“æœï¼Œç¡®ä¿æ¯ä¸ªç« èŠ‚ç´¢å¼•å”¯ä¸€ã€‚"""
        logger.info("--- Running in Deduplicate Results Mode ---")
        logger.info("Loading all existing result files...")
        
        results = self.file_manager.load_existing_results()
        
        if not results or not results.get('chapters'):
            logger.warning("No results found to deduplicate. Exiting.")
            return
            
        all_chapters = results.get('chapters', [])
        logger.info(f"Found {len(all_chapters)} total chapter entries. Starting deduplication...")

        unique_chapters = {}
        for chapter in all_chapters:
            index = chapter.get('chapter_index')
            if index is not None:
                # åå‡ºç°çš„ä¼šè¦†ç›–å…ˆå‡ºç°çš„ï¼Œä¿ç•™æœ€æ–°çš„
                unique_chapters[index] = chapter
            else:
                logger.warning(f"å‘ç°ç¼ºå°‘ 'chapter_index' çš„ç« èŠ‚æ•°æ®ï¼Œå·²è·³è¿‡: {str(chapter)[:100]}")

        deduplicated_list = list(unique_chapters.values())
        
        if len(deduplicated_list) == len(all_chapters):
            logger.info("âœ… No duplicates found. Your data is clean.")
            return

        logger.warning(f"å»é‡å®Œæˆ: ä» {len(all_chapters)} æ¡è®°å½•ä¸­ä¿ç•™äº† {len(deduplicated_list)} æ¡å”¯ä¸€è®°å½•ã€‚")
        
        results['chapters'] = deduplicated_list
        
        try:
            self.file_manager.save_results(results)
            logger.info("âœ… Deduplicated results have been successfully re-saved.")
        except Exception as e:
            logger.error(f"An error occurred while re-saving deduplicated results: {e}")

    def run_sort_results(self):
        """åŠ è½½ã€æ’åºå¹¶é‡æ–°ä¿å­˜æ‰€æœ‰ç°æœ‰çš„ç»“æœæ–‡ä»¶ï¼Œä»¥ä¿®å¤é¡ºåºã€‚"""
        logger.info("--- Running in Sort Results Mode ---")
        logger.info("Loading all existing result files...")
        
        results = self.file_manager.load_existing_results()
        
        if not results or not results.get('chapters'):
            logger.warning("No results found to sort. Exiting.")
            return
        
        logger.info(f"Found {len(results['chapters'])} chapters. Sorting and re-saving...")
        
        # æ ¸å¿ƒé€»è¾‘ï¼šç›´æ¥è°ƒç”¨save_resultsï¼Œå®ƒå†…éƒ¨å·²ç»åŒ…å«äº†æ’åºåŠŸèƒ½
        try:
            self.file_manager.save_results(results)
            logger.info("âœ… All result files have been successfully sorted and re-saved.")
        except Exception as e:
            logger.error(f"An error occurred while sorting and re-saving results: {e}")
            logger.error(traceback.format_exc())

# ==============================================================================
# 5. å‘½ä»¤è¡Œæ¥å£
# ==============================================================================

class InteractiveMode:
    """å°è£…æ‰€æœ‰äº¤äº’å¼èœå•é€»è¾‘"""
    def __init__(self):
        # å®šä¹‰é»˜è®¤é…ç½®æ¨¡æ¿ï¼Œç¡®ä¿æ‰€æœ‰é”®éƒ½å­˜åœ¨
        default_config = {
            "api_key": "",
            "base_url": "https://api.openai.com/v1",
            "model": "gpt-3.5-turbo",
            "epub_path": "",
            "output": "analysis_results_refactored.json",
            "report": "analysis_report_refactored.md",
            "max_tokens": 3000,
            "timeout": 180.0,
            "delay": 2.0,
            "prompt": self._get_default_prompt(),
            "use_json_schema": False
        }
        loaded_config = load_config_from_file(CONFIG_FILE)
        # åˆå¹¶é…ç½®ï¼šä»¥é»˜è®¤é…ç½®ä¸ºåŸºç¡€ï¼Œç”¨åŠ è½½çš„é…ç½®è¦†ç›–
        self.config_data = default_config
        self.config_data.update(loaded_config)

    def _edit_prompt_in_editor(self):
        """ç”¨å¤–éƒ¨ç¼–è¾‘å™¨ç¼–è¾‘æç¤ºè¯"""
        prompt_file = Path("prompt_editor_temp.txt")
        try:
            # 1. å†™å…¥å½“å‰æç¤ºè¯åˆ°ä¸´æ—¶æ–‡ä»¶
            current_prompt = self.config_data.get('prompt', '')
            with open(prompt_file, 'w', encoding='utf-8') as f:
                f.write(current_prompt)

            logger.info("å³å°†æ‰“å¼€è®°äº‹æœ¬ç¼–è¾‘æç¤ºè¯ã€‚è¯·åœ¨è®°äº‹æœ¬ä¸­ä¿®æ”¹ã€ä¿å­˜ï¼Œç„¶åå…³é—­çª—å£ã€‚")
            time.sleep(2) # è®©ç”¨æˆ·æœ‰æ—¶é—´é˜…è¯»ä¿¡æ¯

            # 2. è°ƒç”¨è®°äº‹æœ¬å¹¶ç­‰å¾…å®ƒå…³é—­
            if platform.system() == "Windows":
                subprocess.run(["notepad.exe", str(prompt_file)], check=True)
            else:
                logger.warning("æ­¤åŠŸèƒ½åœ¨éWindowsç³»ç»Ÿä¸Šå°†å°è¯•æ‰“å¼€é»˜è®¤ç¼–è¾‘å™¨ã€‚")
                editor = os.getenv('EDITOR')
                if editor:
                    subprocess.run([editor, str(prompt_file)], check=True)
                else:
                    logger.error("æ— æ³•æ‰¾åˆ°é»˜è®¤æ–‡æœ¬ç¼–è¾‘å™¨ã€‚è¯·è®¾ç½®æ‚¨çš„ EDITOR ç¯å¢ƒå˜é‡ã€‚")
                    return

            # 3. è¯»å–ä¿®æ”¹åçš„å†…å®¹
            with open(prompt_file, 'r', encoding='utf-8') as f:
                new_prompt = f.read()

            # 4. æ›´æ–°é…ç½®
            if new_prompt.strip() != current_prompt.strip():
                self.config_data['prompt'] = new_prompt
                logger.info("âœ… æç¤ºè¯å·²æ›´æ–°ã€‚")
            else:
                logger.info("æç¤ºè¯æœªå‘ç”Ÿå˜åŒ–ã€‚")

        except FileNotFoundError:
             logger.error(f"âŒ é”™è¯¯ï¼šæ‰¾ä¸åˆ°é»˜è®¤çš„æ–‡æœ¬ç¼–è¾‘å™¨ã€‚åœ¨Windowsä¸Šï¼Œè¯·ç¡®ä¿'notepad.exe'åœ¨ç³»ç»Ÿè·¯å¾„ä¸­ã€‚")
        except Exception as e:
            logger.error(f"âŒ ç¼–è¾‘æç¤ºè¯æ—¶å‘ç”Ÿé”™è¯¯: {e}")
        finally:
            # 5. æ¸…ç†ä¸´æ—¶æ–‡ä»¶
            if prompt_file.exists():
                prompt_file.unlink()

    def run(self):
        """å¯åŠ¨äº¤äº’æ¨¡å¼çš„ä¸»å¾ªç¯"""
        while True:
            self._show_main_menu()
            choice = input("è¯·è¾“å…¥æ‚¨çš„é€‰æ‹©: ").strip()
            if choice == '1':
                self.start_analysis_from_menu()
            elif choice == '2':
                self._show_settings_menu()
            elif choice == '3':
                self._show_help()
            elif choice == '4':
                logger.info("æ„Ÿè°¢ä½¿ç”¨ï¼Œå†è§ï¼")
                break
            else:
                logger.warning("æ— æ•ˆè¾“å…¥ï¼Œè¯·é‡æ–°é€‰æ‹©ã€‚")

    def _get_default_prompt(self) -> str:
        # This is now a method of InteractiveMode to be used by the default_config
        return """
ä½ ç°åœ¨éœ€è¦åˆ†æè¿™ç¯‡å†…å®¹å¹¶å®Œæˆä¸‹é¢çš„ä»»åŠ¡ï¼Œä½ éœ€è¦é€‰å–äººç”Ÿæ„Ÿæ‚Ÿæˆ–è€…æ˜¯æ‹çˆ±æ–¹æ³•ç»éªŒç­‰ï¼Œä¿ç•™ä¸Šä¸‹æ–‡ã€‚å¦‚æœæ²¡æœ‰å¯ä»¥å†é‡‘å¥é‚£é‡Œè®¾ç½®ä¸€ä¸ªç©ºåˆ—è¡¨
è¯·æ³¨æ„ï¼š
1.è¯·å‹¿åŒ…å«æ•…äº‹æƒ…èŠ‚ï¼šæ ·ä¾‹ï¼š
1>è™½ç„¶ä»–ä¸çŸ¥é“åˆ«çš„å°è¯´ä½œè€…æ˜¯æ€ä¹ˆæ ·çš„ï¼Œä½†ä»–å¹³æ—¶å°±å–œæ¬¢å»æ›´å¤šçš„åœ°æ–¹ï¼Œè§æ›´å¤šçš„é£æ™¯ï¼Œè¿™æ ·æ‰èƒ½å†™å‡ºæ¥æ›´åŠ ç²¾å½©çš„æ•…äº‹ã€‚
2>ä¸çŸ¥ä¸è§‰ä¸­ï¼Œè‹ç™½ç²¥çš„å¿ƒæƒ…å˜å¾—ååˆ†å®‰é€¸ï¼Œä¹‹å‰çš„ä¸æ„‰å¿«å·²ç»çƒŸæ¶ˆäº‘æ•£ã€‚
2.ä½ çš„ç›®çš„æ˜¯ä¿å­˜é‡‘å¥è®©äººæ„Ÿè§‰æœ‰ä»·å€¼çš„ï¼Œä¸æ˜¯ä¿ç•™äººç‰©çš„æ— ç”¨å¯¹è¯ç­‰
è¯·ä¸¥æ ¼éµå®ˆä»¥ä¸‹JSONæ ¼å¼è¦æ±‚ï¼ˆè¯·å‹¿åœ¨JSONå†…éƒ¨æ·»åŠ ä»»ä½•æ³¨é‡Šï¼‰ï¼š

<JSON>
{
    "chapter_title": "åœ¨æ­¤å¤„å¡«å†™ç« èŠ‚æ ‡é¢˜",
    "golden_sentences": [
        {
            "sentence": "åœ¨æ­¤å¤„å¡«å†™é»„é‡‘å¥å­çš„çº¯æ–‡æœ¬å†…å®¹",
            "speaker": "åœ¨æ­¤å¤„å¡«å†™å¥å­æ¥æºï¼ˆè§’è‰²å/æ—ç™½ç­‰ï¼Œä½ ä¹Ÿå¯ä»¥æ ¹æ®ä¸Šä¸‹æ–‡åˆ¤æ–­ï¼‰",
            "reason": "åœ¨æ­¤å¤„å¡«å†™é€‰æ‹©è¯¥å¥çš„æ‹çˆ±ç†ç”±"
        }
    ],
    "chapter_summary": "åœ¨æ­¤å¤„å¡«å†™æœ¬ç« çš„æ‹çˆ±ä¸»çº¿æ€»ç»“"
}
</JSON>
ç« èŠ‚å†…å®¹å¦‚ä¸‹ï¼š
"""

    def _show_main_menu(self):
        print("\n" + "="*50)
        print("  EPUBå°è¯´é‡‘å¥åˆ†æå™¨ - äº¤äº’æ¨¡å¼")
        print("="*50)
        print("1. å¼€å§‹åˆ†æ")
        print("2. è®¾ç½®")
        print("3. å¸®åŠ©")
        print("4. é€€å‡º")
        print("-"*50)

    def _show_settings_menu(self):
        """æ˜¾ç¤ºå¹¶å¤„ç†è®¾ç½®èœå•"""
        while True:
            print("\n--- è®¾ç½®èœå• ---")
            # ä¸ºäº†ç¨³å®šçš„é¡ºåºï¼Œå¯¹é”®è¿›è¡Œæ’åº
            all_keys = sorted(self.config_data.keys())
            editable_keys = [k for k in all_keys if k != 'prompt']
            
            for i, key in enumerate(editable_keys, 1):
                value = self.config_data[key]
                display_value = '*****' if 'key' in key and value else value
                if isinstance(value, bool):
                    display_value = "âœ… å·²å¯ç”¨" if value else "âŒ å·²ç¦ç”¨"
                print(f"{i}. {key}: {display_value}")
            
            base_index = len(editable_keys)
            print(f"{base_index + 1}. ç¼–è¾‘æç¤ºè¯ (å°†æ‰“å¼€è®°äº‹æœ¬)")
            print(f"{base_index + 2}. æµ‹è¯•APIè¿æ¥")
            print(f"{base_index + 3}. ä¿å­˜å¹¶è¿”å›ä¸»èœå•")
            print(f"{base_index + 4}. ä¸ä¿å­˜å¹¶è¿”å›ä¸»èœå•")
            
            choice = input("è¯·é€‰æ‹©è¦ä¿®æ”¹çš„é…ç½®é¡¹æˆ–æ“ä½œ: ").strip()
            
            try:
                choice_int = int(choice)
                if 1 <= choice_int <= len(editable_keys):
                    key_to_edit = editable_keys[choice_int - 1]
                    new_value_str = input(f"è¯·è¾“å…¥æ–°çš„ '{key_to_edit}' å€¼: ").strip()
                    
                    # å°è¯•å°†è¾“å…¥è½¬æ¢ä¸ºåŸå§‹å€¼çš„ç±»å‹
                    original_value = self.config_data[key_to_edit]
                    try:
                        if isinstance(original_value, bool):
                            if new_value_str.lower() in ['true', 'yes', 'y', '1', 'on']:
                                self.config_data[key_to_edit] = True
                            elif new_value_str.lower() in ['false', 'no', 'n', '0', 'off']:
                                self.config_data[key_to_edit] = False
                            else:
                                logger.warning("æ— æ•ˆçš„å¸ƒå°”å€¼è¾“å…¥ã€‚è¯·è¾“å…¥ 'yes' æˆ– 'no'ã€‚")
                        elif isinstance(original_value, (int, float)):
                                self.config_data[key_to_edit] = type(original_value)(new_value_str)
                        else:
                                self.config_data[key_to_edit] = new_value_str
                    except ValueError:
                        logger.error(f"è¾“å…¥çš„å€¼ '{new_value_str}' ç±»å‹ä¸æ­£ç¡®ã€‚å·²å¿½ç•¥æ›´æ”¹ã€‚")
                elif choice_int == base_index + 1:
                    self._edit_prompt_in_editor()
                elif choice_int == base_index + 2:
                    # æµ‹è¯•APIè¿æ¥
                    temp_config = Config(self.config_data)
                    api_handler = APIHandler(temp_config, InterruptHandler())
                    api_handler.test_connection()
                    input("æŒ‰å›è½¦é”®ç»§ç»­...")
                elif choice_int == base_index + 3:
                    save_config_to_file(CONFIG_FILE, self.config_data)
                    break
                elif choice_int == base_index + 4:
                    self.__init__() # é‡æ–°åˆå§‹åŒ–ä»¥æ”¾å¼ƒæ›´æ”¹
                    break
                else:
                    logger.warning("æ— æ•ˆé€‰æ‹©ã€‚")
            except ValueError:
                logger.warning("è¯·è¾“å…¥æ•°å­—ã€‚")

    def _show_help(self):
        print("\n--- å¸®åŠ©ä¿¡æ¯ ---")
        print("æœ¬å·¥å…·ç”¨äºåˆ†æEPUBæ ¼å¼çš„å°è¯´ï¼Œæå–æœ‰ä»·å€¼çš„'é‡‘å¥'ã€‚")
        print("\nä¸»è¦åŠŸèƒ½:")
        print("- å¼€å§‹åˆ†æ: æ ¹æ®å½“å‰è®¾ç½®ï¼Œå¯åŠ¨åˆ†ææµç¨‹ã€‚")
        print("- è®¾ç½®: é…ç½®APIå¯†é’¥ã€æ¨¡å‹ã€æ–‡ä»¶è·¯å¾„ç­‰ã€‚")
        print("  - APIå¯†é’¥å’Œæ¨¡å‹æ˜¯å¿…éœ€çš„ã€‚")
        print("  - æ–‡ä»¶è·¯å¾„ç­‰å¯ä»¥æ ¹æ®éœ€è¦é…ç½®ã€‚")
        print("- å‘½ä»¤è¡Œæ¨¡å¼: æ‚¨ä¹Ÿå¯ä»¥é€šè¿‡å‘½ä»¤è¡Œå‚æ•°ç›´æ¥è¿è¡Œåˆ†æï¼Œå…·ä½“å‚æ•°è¯·ä½¿ç”¨ --help æŸ¥çœ‹ã€‚")
        input("\næŒ‰å›è½¦é”®è¿”å›ä¸»èœå•...")

    def start_analysis_from_menu(self):
        """ä»èœå•å¯åŠ¨åˆ†æ"""
        if not self.config_data.get("api_key") or not self.config_data.get("epub_path"):
            logger.error("é”™è¯¯: APIå¯†é’¥å’ŒEPUBæ–‡ä»¶è·¯å¾„æ˜¯å¿…éœ€çš„ã€‚è¯·å…ˆåœ¨'è®¾ç½®'ä¸­é…ç½®ã€‚")
            return
        
        config = Config(self.config_data)
        orchestrator = AnalysisOrchestrator(config)
        
        # åœ¨äº¤äº’æ¨¡å¼ä¸‹ï¼Œé»˜è®¤å¯ç”¨ç»­è·‘
        orchestrator.run_analysis(resume=True)

def main():
    parser = argparse.ArgumentParser(
        description='(Refactored) EPUBå°è¯´é‡‘å¥åˆ†æå™¨',
        formatter_class=argparse.RawTextHelpFormatter
    )
    # è°ƒæ•´å‚æ•°ï¼Œä½¿å…¶å˜ä¸ºå¯é€‰ï¼Œä»¥ä¾¿äº¤äº’æ¨¡å¼å¯ä»¥å¯åŠ¨
    parser.add_argument('epub_path', nargs='?', default=None, help='EPUBæ–‡ä»¶è·¯å¾„ (åœ¨å‘½ä»¤è¡Œæ¨¡å¼ä¸‹ä¸ºå¿…éœ€)')
    parser.add_argument('--api-key', help='OpenAI APIå¯†é’¥')
    parser.add_argument('--base-url', help='APIåŸºç¡€URL')
    parser.add_argument('--output', help='è¾“å‡ºæ–‡ä»¶è·¯å¾„')
    parser.add_argument('--report', help='æŠ¥å‘Šæ–‡ä»¶è·¯å¾„')
    parser.add_argument('--no-resume', action='store_true', help='ä¸ä»ä¸Šæ¬¡ä¸­æ–­å¤„æ¢å¤')
    parser.add_argument('--clean', action='store_true', help='æ¸…ç†æ‰€æœ‰åˆ†ææ–‡ä»¶ï¼ˆè¿›åº¦ã€ç»“æœã€æŠ¥å‘Šï¼‰')
    parser.add_argument('--max-tokens', type=int, help='APIæœ€å¤§tokenæ•°')
    parser.add_argument('--timeout', type=float, help='APIè¶…æ—¶æ—¶é—´ï¼ˆç§’ï¼‰')
    parser.add_argument('--delay', type=float, help='APIè°ƒç”¨é—´éš”ï¼ˆç§’ï¼‰')
    parser.add_argument('--model', help='ä½¿ç”¨çš„AIæ¨¡å‹')
    parser.add_argument('--test-mode', action='store_true', help='æµ‹è¯•æ¨¡å¼ï¼ˆä»…åˆ†æå‰3ç« ï¼‰')
    parser.add_argument('--re-analyze', nargs='+', help='å¼ºåˆ¶é‡æ–°åˆ†æä¸€ä¸ªæˆ–å¤šä¸ªç« èŠ‚ç´¢å¼• (ä¾‹å¦‚: 5 10-15 20)')
    parser.add_argument('--retry-failed', action='store_true', help='ä»…é‡è¯•æ‰€æœ‰ä¹‹å‰å¤±è´¥çš„ç« èŠ‚')
    parser.add_argument('--find-missing', action='store_true', help='Verify and analyze all missing chapters, the most robust resume mode.')
    parser.add_argument('--sort-results', action='store_true', help='Load, sort, and re-save all existing result files to fix chapter order.')
    parser.add_argument('--deduplicate-results', action='store_true', help='Load, deduplicate, and re-save all results to ensure chapter uniqueness.')
    
    args = parser.parse_args()

    # åˆ¤æ–­è¿è¡Œæ¨¡å¼
    # å¦‚æœæä¾›äº†epub_pathæˆ–ä»»ä½•å…¶ä»–ç‰¹å®šæ“ä½œï¼Œåˆ™è¿›å…¥å‘½ä»¤è¡Œæ¨¡å¼
    if args.epub_path or any([args.clean, args.sort_results, args.deduplicate_results]):
        # åœ¨å‘½ä»¤è¡Œæ¨¡å¼ä¸‹ï¼Œepub_pathå’Œapi_keyæ˜¯å¿…éœ€çš„
        if not args.epub_path:
            parser.error("åœ¨å‘½ä»¤è¡Œæ¨¡å¼ä¸‹ï¼Œ'epub_path' æ˜¯å¿…éœ€çš„ã€‚")
        
        # å‘½ä»¤è¡Œå‚æ•°ä¼˜å…ˆäºé…ç½®æ–‡ä»¶
        config_from_file = load_config_from_file(CONFIG_FILE)
        arg_dict = {k: v for k, v in vars(args).items() if v is not None}
        config_from_file.update(arg_dict)

        if not config_from_file.get('api_key'):
             parser.error("åœ¨å‘½ä»¤è¡Œæ¨¡å¼ä¸‹ï¼Œ'--api-key' æ˜¯å¿…éœ€çš„ï¼ˆæˆ–åœ¨config.jsonä¸­æä¾›ï¼‰ã€‚")

        config = Config(config_from_file)
        orchestrator = AnalysisOrchestrator(config)

        if args.clean:
            orchestrator.file_manager.clean_all()
            return

        if args.sort_results:
            orchestrator.run_sort_results()
            return

        if args.deduplicate_results:
            orchestrator.run_deduplicate_results()
            return

        re_analyze_list = []
        if args.re_analyze:
            for item in args.re_analyze:
                if '-' in item:
                    try:
                        start, end = map(int, item.split('-'))
                        re_analyze_list.extend(range(start, end + 1))
                    except ValueError: logger.error(f"æ— æ•ˆçš„ç« èŠ‚èŒƒå›´æ ¼å¼: {item}")
                else:
                    try: re_analyze_list.append(int(item))
                    except ValueError: logger.error(f"æ— æ•ˆçš„ç« èŠ‚ç´¢å¼•: {item}")
        
        orchestrator.run_analysis(
            resume=not args.no_resume,
            test_mode=args.test_mode,
            re_analyze_chapters=re_analyze_list if re_analyze_list else None,
            retry_failed=args.retry_failed,
            find_missing=args.find_missing
        )
    else:
        # è¿›å…¥äº¤äº’æ¨¡å¼
        interactive_mode = InteractiveMode()
        interactive_mode.run()

if __name__ == "__main__":
    main()